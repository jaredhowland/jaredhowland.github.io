---
title: How Scholarly Is Google Scholar? A Comparison of Google Scholar to Library Databases
permalink: /publications/google-scholar-ala/
---

{%- include publication.html -%}

## Abstract

Google Scholar (<span class="caps">GS</span>) was released as a beta product in November of 2004. Since then, <span class="caps">GS</span> has been scrutinized and questioned by many in academia and the library field. Our objectives in undertaking this study were to determine how scholarly <span class="caps">GS</span> is in comparison with traditional library resources and to determine if the scholarliness of materials found in <span class="caps">GS</span> varies across disciplines. We found that <span class="caps">GS</span> is, on average, 17.6% more scholarly than materials found only in library databases and that there is no statistically significant difference between the scholarliness of materials found in <span class="caps">GS</span> across disciplines.

## Introduction

Google Scholar (<span class="caps">GS</span>) was introduced to the world in November of 2004 as a beta product. It has been embraced by students, scholars, and librarians alike. However, <span class="caps">GS</span> has received criticism regarding the breadth and scope of available content. We undertook this study to answer two questions regarding these common criticisms: (1) Are <span class="caps">GS</span> result sets more or less scholarly than licensed library database result sets? and (2) Does the scholarliness of <span class="caps">GS</span> vary across disciplines?

## Literature Review

<span class="caps">GS</span>, which is still branded as a beta version, has not only become a common fixture in library literature but is also becoming ubiquitous in information-seeking behavior of users. <span class="caps">GS</span> was initially met with curiosity and skepticism <sup>[[1](#one), [2](#two), [3](#three)]</sup>. This was followed by a period of systematic study <sup>[[4](#four), [5](#five), [6](#six), [7](#seven)]</sup>. More recently, there has been optimism about <span class="caps">GS</span>’s potential to move us towards Kilgour’s goal of 100% availability of information <sup>[[8](#eight)]</sup>. Librarians find themselves reluctantly acknowledging users’ preferences for one-stop information shopping by giving <span class="caps">GS</span> ever-increasing visibility on their web pages <sup>[[9](#nine)]</sup>. Even as they begrudgingly promote <span class="caps">GS</span>, the debate continues within the information community as to the advisability of guiding users to this tool. The view of critics like Péter Jacsó <sup>[[10](#ten), [11](#eleven), [12](#twelve)]</sup>, who use terms such as “shallowness” and “artificial unintelligence” to describe the program, seems to be giving way to a landscape where respected publishers (*e.g.*, Cambridge) and platforms (*e.g.*, <span class="caps">JSTOR</span>) are now offering links out to <span class="caps">GS</span> for more citations.

Early studies of <span class="caps">GS</span> tried to match citations “hit to hit” in comparison with traditional academic search engines. Jacsó even provided a web site where the curious could compare search results between <span class="caps">GS</span> and the likes of Nature, Wiley or Blackwell <sup>[[13](#thirteen)]</sup>. More recently, studies have appeared that track the “value-added” open access citations that appear uniquely in <span class="caps">GS</span> versus other sources <sup>[[14](#fourteen)]</sup>. However, is comprehensiveness of content the primary indicator of a resource’s usefulness?

Every title from every database may not be in <span class="caps">GS</span>, but that should not be an indictment of <span class="caps">GS</span>’s inability to return scholarly results across disciplines. The algorithms <span class="caps">GS</span> uses to return result sets cannot really be compared to library database algorithms. However, what is returned can be judged for its relevancy and “scholarliness.” Up to this point, studies of <span class="caps">GS</span> have followed the example of Neuhaus et al. (2006), which compared <span class="caps">GS</span> content to forty-seven other databases. This title-by-title and citation-by-citation comparison is a pure numerical measure but neglects to address the efficacy of any particular search or the scholarly nature of content or algorithms in discovering that content.

We felt that a different approach was needed. Rather than measuring what has gone into the database, we have sought, to some degree, to evaluate what comes out as a result of search queries. We have done this by involving subject librarians with knowledge of typical reference questions and using those questions to query both <span class="caps">GS</span> and discipline-specific databases. We then asked the same librarians to judge the search results using a rubric of scholarliness. This notion of scholarliness utilizes a common collection-assessment tool as outlined by Alexander <sup>[[15](#fifteen)]</sup>. This model considers many factors, including accuracy, authority, objectivity, currency, and coverage. In this way we have attempted to inject a qualitative value of <span class="caps">GS</span> results to the ongoing debate.

## Methodology

We selected seven subject librarians from various academic disciplines, humanities, science and social science. Each specialist was blind to the purpose of the study. We requested that they provide us (1) a sample question that they typically receive from students, (2) a structured query to search a library database, and (3) the library database they would use for that particular query.
<table>
    <caption><strong>Table 1:</strong> Academic representation in this study</caption>
  <thead>
    <tr>
      <th>Academic Discipline</th>
      <th>Database Query</th>
      <th><span class="caps">GS</span> Query</th>
      <th>Library Database</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>Science</td>
      <td>(ACL OR “anterior cruciate ligament*”) AND injur* AND (athlet* OR sport OR sports) AND (therap* OR treat* OR rehab*)</td>
      <td>ACL OR “anterior cruciate ~ligament” ~injury ~athlete OR sport ~therapy OR ~treatment OR ~rehabilitation</td>
      <td>SportDiscus</td>
    </tr>
    <tr class="even">
      <td>Science</td>
      <td>lung cancer AND (etiol* OR caus*) AND (cigarette* OR smok* OR nicotine*)</td>
      <td>lung cancer ~etiology OR ~cause ~cigarette OR ~smoking OR ~nicotine</td>
      <td>Medline</td>
    </tr>
    <tr>
      <td>Science</td>
      <td>“dark matter” AND evidence</td>
      <td>“dark matter” evidence</td>
      <td>Applied Science and Technology Abstracts</td>
    </tr>
    <tr class="even">
      <td>Social Science</td>
      <td>(“fast food” OR mcdonald’s OR wendy’s OR “burger king” OR restaurant) AND franchis* AND (knowledge n3 transfer OR “knowledge management” OR train*)</td>
      <td>“fast food” OR mcdonald’s OR wendy’s OR “burger king” OR restaurant ~franchise “knowledge transfer” OR “knowledge management” OR ~train</td>
      <td>Business Source Premier</td>
    </tr>
    <tr>
      <td>Social Science</td>
      <td>(“standardized test*” OR “high stakes test*”) AND (“learning disabilit*” OR Dyslexia OR “learning problem”) AND accommodat*</td>
      <td>“standardized ~test” OR “high stakes ~test” “learning ~disability” OR dyslexia OR “learning problem” ~accomodation</td>
      <td>PsycINFO</td>
    </tr>
    <tr class="even">
      <td>Humanities</td>
      <td>(bilingual* OR L2) AND (child* OR toddler) AND “cognitive development”</td>
      <td>~bilingual OR L2 ~child OR toddler “cognitive development”</td>
      <td>Linguistics and Language Behavior Abstracts</td>
    </tr>
    <tr>
      <td>Humanities</td>
      <td>(memor* OR remembrance OR memoir*) AND (holocaust) AND (Spiegelman OR Maus)</td>
      <td>~memor OR remembrance OR ~memoir holocaust Spiegelman OR Maus</td>
      <td><span class="caps"><span class="caps">JSTOR</span></span></td>
    </tr>
  </tbody>
</table>

We then used their data in two different ways. First, we translated the library database query into an equivalent search string used by <span class="caps">GS</span>. Using the original query and the query translated to work with <span class="caps">GS</span>, we searched both the library database and <span class="caps">GS</span> and retrieved the citations and full text for the first thirty results. We selected thirty results because research has shown that less than one percent of all users ever go to a third page of results and most search engines return about ten results per page <sup>[[16](#sixteen)]</sup>.

Next, we took the citations from the library databases and determined if they could also be found using <span class="caps">GS</span> and took the citations from <span class="caps">GS</span> to see if they could also be found in the library database. This allowed us to calculate the overlap of citations between the library databases and <span class="caps">GS</span>.

We standardized the formatting of the citations and inserted them randomly into a spreadsheet, which contained a rubric that was used to assign a scholarliness score to each of the citations. The rubric contained six criteria, based on Alexander’s (1999) model of evaluating resources, to judge scholarliness: (1) accuracy, (2) authority, (3) objectivity, (4) currency, (5) coverage, and (6) relevancy. These criteria were graded on a scale of 1 (below average) to 3 (above average) and summed to create a total scholarliness score for each citation.

<table>
    <caption><strong>Table 2:</strong> Rubric for grading scholarliness<br/>1 = Below Average Quality; 2 = Average Quality; 3 = Above Average Quality</caption>
  <thead>
    <tr>
      <th>Citation Number</th>
      <th>References</th>
      <th>Accuracy</th>
      <th>Authority</th>
      <th>Objectivity</th>
      <th>Currency</th>
      <th>Coverage</th>
      <th>Relevancy</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>1</td>
      <td>Barnes, J. E., &amp; Hernquist, L. E. (1993). Computer models of colliding galaxies. Physics Today, 46, 54&ndash;61.</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
    </tr>
    <tr class="even">
      <td>2</td>
      <td>Bergstrom, L. (2000). Nonbaryonic dark matter: Observational evidence and detection methods. Reports on Progress in Physics, 63(5), 793&ndash;841.</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
      <td>1 2 3</td>
    </tr>
  </tbody>
</table>

We provided the subject librarians with the full text of each of the citations and asked them to use the rubric to evaluate the scholarliness of the individual citations. After the grading was completed, we were able to group each citation from the subject librarian into one of three categories: (1) the citation was available only in the library database, (2) the citation was available only in <span class="caps">GS</span>, or (3) the citation was available in both the library database and <span class="caps">GS</span>. We have used the term “exclusivity” to describe the three categories.

Once we had grouped the citations by category, we ran a statistical analysis that controlled for the effect of the individual librarian on the total scholarliness score, for the effect of “exclusivity”, and for any interaction there may have been between both librarian and “exclusivity”:

*total scholarliness score = µ + E<sub>i</sub> + L<sub>j</sub> + EL<sub>ij</sub> + ϵ<sub>ijkl</sub>*

Where:

µ = Average total score

*E* = Effect due to “exclusivity” (*i* = 1, 2, 3)

*L* = Effect due to librarian (*j* = 1, 2, … 7)

*EL* = Interaction between “exclusivity” and librarian

ϵ = Error term

Within the context of this formula, *E* controls for any effect due to the “exclusivity” of the citation, where *i* represents each of the three categories of “exclusivity” (*i.e.*, the citation was found only in the database, it was found only in <span class="caps">GS</span>, or it was found in both the database and <span class="caps">GS</span>). Each librarian (*L*) also played a role in the total scholarliness score. One librarian could have provided consistently low scores with another having a tendency toward higher scores. To account for this disparity, each librarian was treated as a factor in the total scholarliness score, where *j* represents each of the seven participants. In short, this formula allowed us to calculate a measure of scholarliness while accounting for differences in where the citations were located and between librarians.

## Results

The mean scholarliness score of citations found only in <span class="caps">GS</span> was 17.6% higher than the score for citations found only in licensed library databases. In fact, across all but one of the tested disciplines, citations found only in <span class="caps">GS</span> had a higher average scholarliness score than citations found only in licensed library databases. The one discipline with a lower score, however, had only two unique citations in the library database, so the exact significance of the scores for that discipline is imprecise. Additionally, the citations found in both <span class="caps">GS</span> and licensed library databases had a higher average score than citations found only in one or the other.

<table>
    <caption><strong>Table 3:</strong> Scholarliness based on “exclusivity” (maximum scholarliness score is 18)</caption>
  <thead>
    <tr>
      <th>Participant</th>
      <th>Found Only in Database Average Score</th>
      <th>Found Only in <span class="caps">GS</span> Average Score</th>
      <th>Percent Change in Scholarliness Score Between the Database and <span class="caps">GS</span></th>
      <th>Found in Both Average Score</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>1</td>
      <td>11.7</td>
      <td>16.1</td>
      <td>36.8%</td>
      <td>13.5</td>
    </tr>
    <tr class="even">
      <td>2</td>
      <td>13.2</td>
      <td>13.8</td>
      <td>4.5%</td>
      <td>14.6</td>
    </tr>
    <tr>
      <td>3</td>
      <td>N/A</td>
      <td>12.0</td>
      <td>N/A</td>
      <td>15.6</td>
    </tr>
    <tr class="even">
      <td>4</td>
      <td>10.0</td>
      <td>13.5</td>
      <td>35.0%</td>
      <td>14.3</td>
    </tr>
    <tr>
      <td>5</td>
      <td>10.0</td>
      <td>11.6</td>
      <td>16.0%</td>
      <td>11.5</td>
    </tr>
    <tr class="even">
      <td>6</td>
      <td>11.7</td>
      <td>12.8</td>
      <td>8.5%</td>
      <td>14.3</td>
    </tr>
    <tr>
      <td>7</td>
      <td>16.5</td>
      <td>14.4</td>
      <td>−12.7%</td>
      <td>13.9</td>
    </tr>
    <tr class="even">
      <td><strong>Least Squares Mean</strong></td>
      <td><strong>11.9</strong></td>
      <td><strong>14.0</strong></td>
      <td><strong>17.6%</strong></td>
      <td><strong>14.2</strong></td>
    </tr>
  </tbody>
</table>

Finally, there was no statistically significant difference found between the scholarliness score across disciplines within <span class="caps">GS</span>. Searching for either a humanities topic or a science topic yielded no difference in the scholarliness score of citations discovered in <span class="caps">GS</span>.

## Discussion of Results

It is interesting to note that there was very little overlap between the initial thirty citations returned by the databases and the initial thirty citations returned by <span class="caps">GS</span>. In fact, only one query of the seven had any overlapping citations between <span class="caps">GS</span> and the database— an overlap of five citations from <span class="caps">JSTOR</span> that appeared within the first thirty results in <span class="caps">GS</span>. Despite this initial lack of overlap, once we began to search for specific citations, we found that <span class="caps">GS</span> actually contained 76% of all the citations found in the library databases, while the library databases contained only 47% of the citations found in <span class="caps">GS</span>.

<table>
    <caption><strong>Table 4:</strong> Overlap of citations</caption>
  <thead>
    <tr>
      <th>Participant</th>
      <th>Percent of database citations in <span class="caps">GS</span></th>
      <th>Percent of <span class="caps">GS</span> citations in database</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>1</td>
      <td>76.6%</td>
      <td>0.0%</td>
    </tr>
    <tr class="even">
      <td>2</td>
      <td>83.3%</td>
      <td>43.3%</td>
    </tr>
    <tr>
      <td>3</td>
      <td>100.0%</td>
      <td>96.7%</td>
    </tr>
    <tr class="even">
      <td>4</td>
      <td>96.7%</td>
      <td>80.0%</td>
    </tr>
    <tr>
      <td>5</td>
      <td>93.3%</td>
      <td>28.0%</td>
    </tr>
    <tr class="even">
      <td>6</td>
      <td>0.0%</td>
      <td>46.7%</td>
    </tr>
    <tr>
      <td>7</td>
      <td>81.8%</td>
      <td>34.5%</td>
    </tr>
    <tr class="even">
      <td><strong>Average</strong></td>
      <td><strong>76.0%</strong></td>
      <td><strong>47.0%</strong></td>
    </tr>
  </tbody>
</table>

This seems to validate the decision of many students to use Google first to look for information. If <span class="caps">GS</span> contains much of the content available in library databases, why shouldn’t students begin where the most content exists? The argument is made that <span class="caps">GS</span> will return millions of hits, many of which are spurious at best, while a library database will only return a few thousand results that are more focused to the query.

However, the power of ordering results by relevancy, combined with the fact that very few people ever go beyond the first page of results, creates a searcher-imposed higher level of precision for any search engine. This is particularly true of <span class="caps">GS</span>, where the most relevant and more scholarly, material floats to the top of the list, while the less precise material falls to the bottom, where it is rarely seen. Hit counts are of secondary importance in a <span class="caps">GS</span> search; the key to <span class="caps">GS</span>’s success is relevancy ranking and a large universe of information.

A database is limited to its defined title list of content, whereas <span class="caps">GS</span>, by its very nature, is open to a much broader set of content that aids the researcher. Business Source Premier, one of the library databases, was the only library database where we found more <span class="caps">GS</span> citations in the database than database citations in <span class="caps">GS</span>.  However, even in this one instance, the scholarly score for citations found only in <span class="caps">GS</span> was higher than the score for citations found only in the database. The citations found in both <span class="caps">GS</span> and the databases received even higher scores and these citations were only exposed through the first 30 hits in <span class="caps">GS</span>. This seems to indicate that even when <span class="caps">GS</span> is returning fewer titles, as in this case with Business Source Premier, it still returns citations that are more scholarly to the top.

Up to this point, many of our library databases have defaulted to sorting by date rather than by relevancy. The fact that many databases are now adding relevancy search options seems to indicate that <span class="caps">GS</span> got it right in the first place. It appears that <span class="caps">GS</span> has done a better job of both precision and recall than library databases have.

Many studies have compared content in library databases to content in <span class="caps">GS</span> and found inconsistencies. The purpose of both search systems, however, is to discover relevant, scholarly content. Using our scholarliness model, we found that, across disciplines, <span class="caps">GS</span> is generally superior to individual databases in retrieving appropriate citations. As more publishers share their content with <span class="caps">GS</span>, we would expect the effectiveness of a <span class="caps">GS</span> search to increase.

## Future Studies

The statistical results from this study can be extrapolated only to the specific topics and subject librarians that were involved in the study. A more comprehensive statistical methodology would need to be constructed in order to make the results generally applicable. However, our results were compelling enough to make us believe that the results would hold up to more strenuous tests. Additionally, the rubric we used in our study was only a three-point Likert scale. Finding statistically significant differences would have been easier had we selected a seven or more point Likert scale.

Additionally, our analysis used a vetted approach to evaluating scholarliness of resources. A more objective view of scholarliness could be obtained by using some variation of citation analysis (citation counts, <span class="caps">ISI</span> impact factor, etc.). We started to do such an analysis but decided there were too many trade-offs to be appropriate given the methodology we used for this study. For example, citation counts are difficult to come by for materials other than journal articles, and impact factors are calculated for journals only and not for specific articles. Alternate methodologies would likely be able to overcome or account for the shortcomings of using citation analysis to judge scholarliness.

Our study used skilled librarians to create search queries and to judge the quality of the citations retrieved. Unlike most students, the librarians used complex search queries to find more relevant results. Students would be more likely to use natural language queries to find citations. Complex search queries could return very different results from natural language queries. Future studies will need to address the potential differences to find out if the results we found hold across different types of searches.

Finally, future studies need to look at the appropriateness of comparing <span class="caps">GS</span> to individual library databases. It is probable that federated searching is more comparable to <span class="caps">GS</span> than are individual library databases. However, how users and librarians select which resources to use in a federated search and how the federated search engine returns the results would still impact the discoverability of scholarly resources. Some studies have already started down this road <sup>[[17](#seventeen)]</sup>, but <span class="caps">GS</span> result sets have still not been carefully compared to result sets from federated search products.

Libraries have begun to build local <span class="caps">GS</span>’s, using tools such as Primo (Ex Libris), AquaBrowser (Medialab Solutions), and Encore (Innovative Interfaces), that have the potential to aid users in discovering even more scholarly materials than what is currently discovered in <span class="caps">GS</span>. Comparing <span class="caps">GS</span> to a future system that has completely indexed all local content and content available to libraries but provided by third parties would be the ultimate comparison.

## Conclusion

Typical arguments against <span class="caps">GS</span> focus on citation counts and point to inconsistent coverage between disciplines. We felt the more appropriate analysis was to compare the scholarliness of resources discovered using <span class="caps">GS</span> with resources found in library databases. This analysis showed that <span class="caps">GS</span> yielded more scholarly content than library databases, with no statistically significant difference in scholarliness across disciplines. Despite these findings, <span class="caps">GS</span> is not in competition with library databases. In truth, without the cooperation of database vendors and publishers, <span class="caps">GS</span> would not exist as it does today. <span class="caps">GS</span> is simply a discovery tool for finding scholarly information while databases still perform the function of providing access to the content unearthed by a <span class="caps">GS</span> search.

## Bibliography
1. <a name="one"></a>Brophy J, Bawden D. Is Google enough? Comparison of an Internet Search Engine with Academic Library Resources. *Perspectives* **2005**; 57:498–512.
2. <a name="two"></a>Gardner S, Eng S. Gaga over Google? Scholar in the Social Sciences. *Library Hi Tech News* **2005**; 22:42–5.
3. <a name="three"></a>Kesselman M, Watstein SB. Google Scholar and Libraries: Point/Counterpoint. *Reference Services Review* **2005**; 33:380–7.
4. <a name="four"></a>Bakkalbasi N, Bauer K, Glover J, Wang L. Three options for citation tracking: Google Scholar, Scopus and Web of Science. *Biomedical Digital Libraries* **2006**; 3:7.
5. <a name="five"></a>Kousha K, Thelwall M. Google Scholar and Google Web/URL Citations: A Multi-Discipline Exploratory Analysis. *Journal of the American Society for Information Science and Technology* **2007**;58: 1055–65.
6. <a name="six"></a>Robinson ML, Wusteman J. Putting Google Scholar to the Test: A Preliminary Study. *Program: Electronic Library & Information Systems* **2007**; 41:71–80.
7. <a name="seven"></a>Neuhaus C, Neuhaus E, Asher A, Wrede C. The Depth and Breadth of Google Scholar: An Empirical Study. *Portal: Libraries and the Academy* **2006**; 6:127–41.
8. <a name="eight"></a>Pomerantz J. Google Scholar and 100 Percent Availability of Information. *Information Technology and Libraries*, June **2006**; 52–6.
9. <a name="nine"></a>Mullen LB, Hartman KA. Google Scholar and the Library Web Site: The Early Response by ARL Libraries. *College and Research Libraries* **2006**; 67:106–22.
10. <a name="ten"></a>Jacsó P. As We May Search - Comparison of Major Features of the Web of Science, Scopus, and Google Scholar Citation-based and Citation-enhanced Databases. *Current Science* **2005**; 89:1537–47.
11. <a name="eleven"></a>Jacsó P. Google Scholar: The Pros and the Cons. *Online Information Review* **2005**; 29:208–14.
12. <a name="twelve"></a>Jacsó P. Deflated, Inflated and Phantom Citation Counts. *Online Information Review* **2006**; 30:297–309.
13. <a name="thirteen"></a>Péter Jacsó: side-by-side2 - Google Scholar vs Native Search. Available at: <http://www2.hawaii.edu/~jacso/scholarly/side-by-side2.htm>. Accessed March 24, 2008.
14. <a name="fourteen"></a>Kousha K, Thelwall M. How is Science Cited on the Web? A Classification of Google Unique Web Citations. *Journal of the American Society for Information Science and Technology* **2007**; 58:1631–44.
15. <a name="fifteen"></a>Alexander JE. *Web Wisdom: How to Evaluate and Create Information Quality on the Web*. Mahwah N.J.: Lawrence Erlbaum Associates Publishers; **1999**.
16. <a name="sixteen"></a>Nielsen J, Loranger H. *Prioritizing Web Usability*. Berkeley, <span class="caps">CA</span>.: New Riders; **2006**.
17. <a name="seventeen"></a>Chen, X. MetaLib, WebFeat, and Google: The strengths and weaknesses of federated search engines compared with Google. *Online Information Review* **2006**; 30:413–27.
